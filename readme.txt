结论

在本文中，我们看到了使用Hyperopt实现基于贝叶斯模型的优化的基本实现。这需要四个部分:

目标:我们想要最小化的东西

域:使目标最小化的参数值

超参数优化函数:如何构建代理函数和下一个值

由分数、参数对组成的试验

随机搜索和基于顺序模型的优化之间的区别很明显:随机搜索是不知情的，因此需要更多的试验来最小化目标函数。树Parzen估计器(一种用于SMBO的算法)花费了更多的时间选择下一个值，但是总体上需要对目标函数进行更少的评估，因为它能够推断出下一个要评估的值。经过多次迭代，SMBO算法集中在最有前途的值周围进行搜索，结果如下:

目标函数得分较低

更快的优化

基于贝叶斯模型的优化意味着构建一个概率模型

??(??|??)

p(y | x)

并对模型进行了更新，收集了更多的信息。随着评价次数的增加，模型(也称为代理函数)成为目标函数更精确的描述，算法花费更多的时间来评估有价值的值。

这个笔记本展示了使用Hyperopt实现的基于贝叶斯模型的优化的基本实现，但是我们已经可以看到它如何比基于随机或网格搜索的方法有显著的优势。在未来的笔记中，我们将探索使用基于贝叶斯模型的优化更复杂的问题，即机器学习模型的超参数优化。